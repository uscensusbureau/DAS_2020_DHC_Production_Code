# Main file for US+PR DHCH runs with manual topdown

[DEFAULT]
# root specifies the root location for all files; testdir specifies ???; mode specifies ???
# For the demo, the root in the current directory
include=../default.ini

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[environment]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[python]

[gurobi]
gurobi_logfile_name: gurobi.log
OutputFlag: 1
OptimalityTol: 1e-4
BarConvTol: 1e-8
BarQCPConvTol: 0
BarIterLimit: 1000
FeasibilityTol: 1e-9
Threads: 4
Presolve: -1
NumericFocus: 3

# Method controls alg used. -1=automatic, 0=primal simplex, 1=dual simplex, 2=barrier
Method: 0

# TimeLimit: 1800
# Do we explicitly run presolve in Python?  1 or 0
python_presolve: 1


[geodict]
# Geolevel_names and geolevel_lengs are implemented in inherited files, as they are different for US and for PR
#smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,Tract_Group,County,State,US
#(largest geocode length to smallest, put 0 for US or US+PR (i.e. above state) level)
geolevel_leng: 16,14,11,8,5,2,0
geocode_length: 16
;geo_toplevel =
geo_bottomlevel: Block
;geo_path =

spine= opt_spine

# The first recommendation from the Geography Division for the choice parameter aian_areas is
aian_areas= Legal_Federally_Recognized_American_Indian_Area,American_Indian_Joint_Use_Area,Hawaiian_Home_Land,Alaska_Native_Village_Statistical_Area,State_Recognized_Legal_American_Indian_Area,Oklahoma_Tribal_Statistical_Area,Joint_Use_Oklahoma_Tribal_Statistical_Area

[setup]
setup: programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name: DAS_NAT_TEST_PRODUCT
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
INCLUDE=Reader/unit_2020.ini
Household.path: $DAS_S3INPUTS/title13_input_data/table14/
Unit.path: $DAS_S3INPUTS/title13_input_data/table10_20190610/

numReaderPartitions: 5000
readerPartitionLen: 12
validate_input_data_constraints: False
input_data_vintage: 2010

[engine]
engine: programs.engine.topdown_engine.TopdownEngine

# should we delete the true data after making DP measurments (1 for True or 0 for False)
delete_raw: 0
save_noisy: 1
reload_noisy: 1
check_budget: off

[schema]
schema: Household2010

[budget]
global_scale: 1/1
privacy_framework: zcdp
#dp_mechanism: discrete_gaussian_mechanism
dp_mechanism: rounded_continuous_gaussian_mechanism
print_per_attr_epsilons: True

#budget in topdown order (e.g. US, State, .... , Block)
geolevel_budget_prop: 0.15,0.15,0.15,0.15,0.15,.15,0.1


[constraints]
#start with none
#the invariants created, (no spaces)
#theInvariants.Block: tot_hu,gqhh_vect
#theInvariants.Tract:

#these are the info to build cenquery.constraint objects
#theConstraints.Block: total,no_vacant,living_alone,size2,size3,size4,size2plus_notalone,not_multigen,hh_elderly,age_child
#theConstraints.Tract:

#Note: you apparently need to specify this now
#minimalSchema: sex

[writer]
writer: programs.writer.mdf2020writer.MDF2020HouseholdWriter

# Where the data gets written:
output_path: $DAS_S3ROOT/DHC_TestProduct/TestMUD/

# Save the output:
produce_flag: 1

# delete existing file (if one) 0 or 1
overwrite_flag: 1

# upload the logfile to the dashboard:
upload_logfile: 1

classification_level: C_U_I//CENS
output_datafile_name: MDF_UNIT
write_metadata: 1
s3cat: 1
s3cat_suffix: .csv
s3cat_verbose: 0

[validator]
validator: programs.stub_validator.validator

#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/WNS_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: 0

[experiment]
experiment: programs.experiment.experiment.experiment
run_experiment_flag: 0

[error_metrics]
#error_metrics: programs.metrics.accuracy_metrics.AccuracyMetrics
error_metrics: programs.metrics.error_metrics_stub.ErrorMetricsStub
