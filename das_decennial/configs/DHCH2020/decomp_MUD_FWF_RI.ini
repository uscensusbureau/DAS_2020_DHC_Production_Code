# RI test for DHCH

[DEFAULT]
INCLUDE=MUD_FWF.ini

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs
dvs_enabled: False
; these environment variables automatically created on the MASTER and CORE nodes.
[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]
# smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,County,State
#(largest geocode length to smallest, put 0 for US or US+PR (i.e. above state) level)
geolevel_leng: 16,12,11,5,2
spine: opt_spine
# The first reccomendation from the Geography Division for the choice parameter aian_areas is:
aian_areas:Legal_Federally_Recognized_American_Indian_Area,American_Indian_Joint_Use_Area,Hawaiian_Home_Land,Alaska_Native_Village_Statistical_Area,State_Recognized_Legal_American_Indian_Area,Oklahoma_Tribal_Statistical_Area,Joint_Use_Oklahoma_Tribal_Statistical_Area

[setup]
#setup: programs.das_setup.DASDecennialSetup
# Spark config stuff
spark.name: DAS_RI_TEST
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
Household.path:  $DAS_S3INPUTS/2010-convert/cef/us/unit/CEF20_UNIT_44.txt
Unit.path:  $DAS_S3INPUTS/2010-convert/cef/us/unit/CEF20_UNIT_44.txt
;Household.path:  $DAS_S3INPUTS/2010-convert/cef/us/unit/CEF20_UNIT_44.txt
;Unit.path:  $DAS_S3INPUTS/2010-convert/cef/us/unit/CEF20_UNIT_44.txt
;grfc_path: $DAS_S3ROOT/2020/cef_fwf/full/grfc.csv
grfc_path: $DAS_S3ROOT/2010-convert-input/grfc/grfc_tab20_[0-9]*.txt
numReaderPartitions: 20000
#validate_input_data_constraints: True

[budget]
privacy_framework= zcdp
dp_mechanism= rounded_continuous_gaussian_mechanism
print_per_attr_epsilons= True
global_scale= 1/1
#epsilon_budget_total= 4/1
#budget in topdown order (e.g. US+PR, State, .... , Block)
geolevel_budget_prop= 1/5,1/5,1/5,1/5,1/5
strategy: decomp_test_strategy_dhch
query_ordering: decomp_test_strategy_regular_ordering_dhch

[error_metrics]
#error_metrics: programs.metrics.accuracy_metrics_workload.AccuracyMetricsWorkload
error_metrics: programs.metrics.accuracy_metrics.AccuracyMetrics
#l1_relative_error_queries: cenrace_7lev_two_comb * hispanic, cenrace * hispanic
#l1_relative_error_geolevels: Place, Block_Group
#population_cutoff: 500
calculate_per_query_quantile_errors = False
calculate_per_query_quantile_signed_errors = False
calculate_binned_query_errors = False

[gurobi]
threads_root2root: 64
threads_state: 64
threads_county: 15
threads_tract_group: 5
threads_tract: 5
threads_block_group: 1
threads_block: 1

OptimalityTol: 1e-7
BarConvTol: 1e-8
BarIterLimit: 1000
FeasibilityTol: 1e-7
Presolve: 2
NumericFocus: 2

method=-1

L2_acceptable_statuses=SUBOPTIMAL,ITERATION_LIMIT
Rounder_acceptable_statuses=SUBOPTIMAL,ITERATION_LIMIT

# Do we explicitly run presolve in Python?  1 or 0
python_presolve: 0
# implemented functions are:
# threshMaxTailGeo, threshBH, threshwigglesum, threshwiggleadd
threshold_function = threshwigglesum
BHfdr = 0.1
wigglesummult = 1.
maxtaildelta = 0.01

seq_optimization_approach = L2PlusRounder_interleaved
#outer_pass = True
l2_optimization_approach = DataIndUserSpecifiedQueriesNPassDecomp
#Decomp
rounder_optimization_approach = MultipassRounderDecomp
#Decomp
#rounder_optimization_approach = MultipassQueryRounder

DataIndNPass_toleranceType=opt_tol
opt_tol_slack=0.01
test_hist_decomp=False
skip_decomp=True

[python]
[engine]
save_noisy=on
[schema]
[writer]
output_path: $DAS_S3ROOT/users/$JBID/test_cef_runs/20200710
[validator]
[assessment]
[takedown]
[experiment]
[constraints]
