# SUCH_BAT
[alert]
message = Hello world!

[budget]
dp_mechanism = discrete_gaussian_mechanism
epsilon_budget_total =
geolevel_budget_prop =
global_scale =
only_dyadic_rationals = False
print_per_attr_epsilons = True
privacy_framework = zcdp
query_ordering = test_strategy_regular_ordering_dhch_20220922_exp_iteration_23_1
strategy = test_strategy_dhch_pr_20220922_exp_iteration_23_1

[constraints]
theconstraints.block = h1, gq_vect, living_alone, size2, size3, size4, hh_elderly, age_child, owned
theconstraints.state = h1, gq_vect, living_alone, size2, size3, size4, hh_elderly, age_child, owned
theconstraints.us = h1, gq_vect, living_alone, size2, size3, size4, hh_elderly, age_child, owned
theinvariants.block = tot, tot_hu, gqhh_vect

[engine]
aian_sum_constraints = False
check_budget = off
delete_raw = 0
engine = programs.engine.topdown_engine.TopdownEngine
geolevel_num_part = 10000,2000,2000,1000,400,40,1
part_size_noisy = 10
part_size_optimized = 10
part_size_optimized_block = 70
postprocess_only = off
reload_noisy = 1
repartition_optimized_node_rdd_evenly = True
save_noisy = on
skip_persists_in_engine = True

[environment]
aws_auto_scaling_home = /opt/aws/apitools/as
aws_cloudwatch_home = /opt/aws/apitools/mon
aws_default_output = text
aws_elb_home = /opt/aws/apitools/elb
aws_no_proxy =
aws_path = /opt/aws
aws_proxy =
census_das_env_included = yes
das_cloud = CB
das_dashboard_unsafe_tls =
das_dashboard_url = https://{HOST_NAME}
das_framework_version = 0.0.1
das_home = /mnt/gits/das-vm-config
das_jobid = DAS-ITECB-043-hermo-DHC-Tests
das_loghost = {HOST_NAME} 
das_log_endpoint = https://{HOST_NAME}/api/daslog/send
das_log_endpoint_debug = https://{HOST_NAME}/api/daslog/send
das_no_bootstrap = false
das_object_cache_base =
das_pythondir = /mnt/das_python
das_rel_root = ${DAS_S3MGMT}/release/update-drpng-1.1.1
das_release = update-drpng-1.1.1
das_run_type = Testing
das_s3inputs = $DAS_S3INPUTS
das_s3logs = $DAS_S3LOGS
das_s3mgmt = $DAS_S3MGMT
das_s3root = $DAS_S3ROOT
das_soa = https://{HOST_NAME}/mft/request/soap
das_soa_user = ITE-DAS
das_testpoint_logger = /mnt/gits/das-vm-config/bin/das_testpoints.py
das_tier = ITE
dvs_object_cache_base =
emr_release = emr-6.2.1
git_ssl_no_verify = true
grb_app_name = DAS
grb_env3 = 0
grb_env4 =
grb_isv_name = Census
grb_license_file = $GUROBI_HOME/gurobi_client.lic
grb_tokenserver = $GRB_TOKENSERVER
grb_tokenserver_port = $GRB_TOKENSERVER_PORT
gurobi_home = /usr/local/lib64/python3.6/site-packages/gurobipy
ismaster = true
java_home = /etc/alternatives/jre
json_config_source = /emr/instance-controller/lib/bootstrap-actions/1/das_config.json
ld_library_path = /usr/local/lib64/python3.6/site-packages/gurobipy/lib:/usr/local/lib64/python3.6/site-packages/gurobipy/lib::/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native
mdf_name =
nmf_name =
pip = pip-3.7
pyspark_driver_python = /usr/bin/python3
pyspark_python = /usr/bin/python3
python = /usr/bin/python3
pythonlib = /usr/lib/python3.7
pythonpath = /mnt/das_python
skip_snmp = yes
skip_splunk = yes
temp = /usr/tmp
tmp = /usr/tmp
tmpdir = /usr/tmp
tz = America/New_York

[error_metrics]
calculate_binned_query_errors = False
compute_prim_error_metrics = True
error_metrics = programs.metrics.accuracy_metrics.AccuracyMetrics
print_place_mcd_ose_bg_l1_error_on_total_pop = True
queries2measure = hhsize_all, presence75, size1, presence65, dhch_hhtype_p12_part2 * sex, dhch_pco3_margin2, hisp, dhch_p16_margin, size1_size2plus, dhch_hhtype_p12_part1, dhch_pco3_margin3, coupled_hh_type, hhtenshort_3lev, dhch_pco3_margin1, race, dhch_hhtype_p12_part3 * sex, dhch_hhtype_hct3, multig, dhch_pco3_margin2 * sex, hhtype_dhch_married_fam_pco11_p1, sex * popSehsdTarget_upart, sex, hhage, presence60, multig * hisp * hhtenshort_3lev, partner_type_own_child_status * sex * hhtenshort_3lev, coupled_hh_type * hisp * hhtenshort_3lev
skip_levels = Block

[experiment]
experiment = programs.experiment.experiment.experiment
run_experiment_flag = 0

[geodict]
aian_areas = Legal_Federally_Recognized_American_Indian_Area,American_Indian_Joint_Use_Area,Hawaiian_Home_Land,Alaska_Native_Village_Statistical_Area,State_Recognized_Legal_American_Indian_Area,Oklahoma_Tribal_Statistical_Area,Joint_Use_Oklahoma_Tribal_Statistical_Area
geocode_length = 16
geolevel_leng = 16,13,12,11,6,5,2
geolevel_names =
geo_bottomlevel = Block
geo_path =
geo_toplevel =
prim_spine = True
spine = opt_spine
spine.re = ([a-z_]+)
spine.required = True
target_das_aian_areas = True
target_orig_block_groups = 1
target_school_dists = True
prim_geo_s3_path = ${DAS_S3ROOT}/${WORKING_PATH}/${PRIM_GEO_S3_PATH}
use_prim_crosswalk=True

[gurobi]
barconvtol = 1e-10
bariterlimit = 1000
barqcpconvtol = 0
bhfdr = 0.1
constrain_main_vars_to_zero = True
dataindnpass_tolerancetype = opt_tol
feasibilitytol = 1e-8
gurobi_lic = $GUROBI_HOME/gurobi_client.lic
gurobi_lic_create = true
gurobi_logfile_name = gurobi.log
gurobi_path = $GUROBI_HOME/linux64/lib/${PYTHON_VERSION}_utf32/
heartbeat_frequency = 0
l2_acceptable_statuses = SUBOPTIMAL,ITERATION_LIMIT
l2_grb_algorithm = -1
l2_grb_presolve = -1
l2_grb_presparsify = -1
l2_optimization_approach = DataIndUserSpecifiedQueriesNPassDecomp
l2_suboptimal_allowed = False
maxtaildelta = 0.01
method = -1
mipfocus = 1
notification_frequency = 120
numericfocus = 3
optimalitytol = 1e-8
opt_tol_slack = 0.01
outputflag = 1
port = $GRB_TOKENSERVER_PORT
presolve = 2
print_gurobi_stats = False
python_presolve = 0
random_report_frequency = 0.0
record_gurobi_stats = True
rounder_acceptable_statuses = SUBOPTIMAL,ITERATION_LIMIT
rounder_optimization_approach = MultipassRounderDecomp
save_lp_seconds = 10000
seq_optimization_approach = L2PlusRounder_interleaved
skip_decomp = True
test_hist_decomp = False
threads = 4
threads_block = 3
threads_block_group = 4
threads_county = 16
threads_prim = 6
threads_root2root = 32
threads_state = 32
threads_tract = 4
threads_tract_group = 8
threads_tract_subset = 4
threads_tract_subset_group = 4
tokenserver = $GRB_TOKENSERVER

[logging]
dvs_api_endpoint = https://{HOST_NAME}/api/dvs
dvs_enabled = False
logfilename = DAS
logfolder = logs
loglevel = INFO
testpoint_stepid=T04

[monitoring]
heartbeat_frequency = 60
notification_frequency = 60
notify_dashboard_gurobi_retry = true
notify_dashboard_gurobi_success = false
print_heartbeat = False
print_heartbeat_frequency = 180
send_stacktrace = False

[reader]
comment = #
constraint_tables = h1
delimiter = |
elderly = p60 p65 p75
elderly.legal = 0-3
elderly.type = int
geocode.legal = 0000000000000000-9999999999999999
geocode.type = str
geocode_h1 = TABBLKST TABBLKCOU TABTRACTCE TABBLKGRPCE TABBLK
geocode_h1.legal = 0000000000000000-9999999999999999
geocode_h1.type = str
git_commit = das_framework commit 53e0a383c6556ee275c87454d4dc9ca8c47a1f23 |ctools commit eb166b94676fb9c77fb3a6a45ebdf646861cfa4c |python_dvs commit 1f6f401fe67889d3994ba157af1d4ff8cc15124c |das_decennial commit c4a9cb7c09061a3ca89639f40f3342881bc2506c
gqtype.legal = 000-999
gqtype.type = str
grfc_path = $GRFC_SINGLE_STATE_PATH
h1.class = programs.reader.sql_spar_table.SQLSparseHistogramTable
h1.geography = geocode_h1
h1.histogram = HHSTATUS
h1.newrecoder = True
h1.path = $H1_Working_PR
h1.recode_variables = geocode_h1
h1.recoder = programs.reader.from_mdf_recoder.DHCRecoder
h1.variables = SCHEMA_TYPE_CODE SCHEMA_BUILD_ID TABBLKST TABBLKCOU TABTRACTCE TABBLKGRPCE TABBLK RTYPE HHSTATUS
header = True
hhage = hhldrage
hhage.legal = 0-8
hhage.type = int
hhgq = gqtype vacs
hhgq.legal = 0-29
hhgq.type = int
hhstatus.legal = 0-2
hhstatus.type = str
hhtenshort = ten
hhtenshort.legal = 0-1
hhtenshort.type = int
hhtenshort_3lev = ten
hhtenshort_3lev.legal = 0-2
hhtenshort_3lev.type = int
hhtype.legal = 0-521
hhtype.type = int
hhtype_dhch = hht
hhtype_dhch.legal = 0-521
hhtype_dhch.type = int
hisp = hhspan
hisp.legal = 0-1
hisp.type = int
household.class = programs.reader.cef_2020.cef_2020_dhch_reader.CEF2020DHCHHouseholdTable
household.generated_module = programs.reader.cef_2020.cef_validator_classes
household.generated_table = CEF20_UNIT
household.geography = geocode
household.histogram = sex hhage hisp race elderly hhtenshort_3lev hhtype_dhch
household.path = $CEF_UNIT_SINGLE_STATE_PATH
household.recode_variables = sex hhage hisp race elderly hhtenshort_3lev hhtype_dhch
household.recoder = programs.reader.cef_2020.cef_2020_dhch_reader.DHCH_Household_recoder_Ten_3Lev
household.variables = mafid hisp sex ten geocode race elderly hhage hhtype
input_data_vintage = 2010
linkage = geocode
mafid.legal = 000000000-999999999
mafid.type = str
main_table = Household
numreaderpartitions = 1000
parts_after_reading = 10000
person.path = $CEF_PER_SINGLE_STATE_PATH
privacy_table = Household
race = hhrace
race.legal = 0-6
race.type = int
reader = programs.reader.table_reader.DASDecennialReader
readerpartitionlen = 15
rtype.legal = 2,4
rtype.type = str
schema_build_id.legal = 0.0.0-9.9.9
schema_build_id.type = str
schema_type_code.legal = AAA-ZZZ
schema_type_code.type = str
sex = hhsex
sex.legal = 0-1
sex.type = int
skip_persist_in_reader = True
tabblk.legal = 0001-9999
tabblk.type = str
tabblkcou.legal = 000-840
tabblkcou.type = str
tabblkgrpce.legal = 0-9
tabblkgrpce.type = str
tabblkst.legal = 01-02,04-06,08-13,15-42,44-51,53-56,72
tabblkst.type = str
tables = Household Unit h1
tabtractce.legal = 000000-998999
tabtractce.type = str
ten.legal = 0-3
ten.type = int
tenvacgq = ten vacs qgqtyp
tenvacgq.legal = 0-34
tenvacgq.type = int
unit.class = programs.reader.cef_2020.cef_2020_dhch_reader.CEF2020DHCHUnitTable
unit.generated_module = programs.reader.cef_2020.cef_validator_classes
unit.generated_table = CEF20_UNIT
unit.geography = geocode
unit.histogram = tenvacgq
unit.path = $CEF_UNIT_SINGLE_STATE_PATH
unit.recode_variables = tenvacgq
unit.recoder = programs.reader.cef_2020.cef_2020_dhch_reader.DHCH_Unit_recoder
unit.variables = MAFID ten vacs gqtype geocode
unit_table = Unit
vacs.legal = 0-7
vacs.type = int
validate_input_data_constraints = False

[schema]
schema = DHCH_SCHEMA_TEN_3LEV

[setup]
environment = AWS_AUTO_SCALING_HOME,AWS_DEFAULT_REGION,AWS_CLOUDWATCH_HOME,AWS_ELB_HOME,AWS_PATH,BCC_HTTP_PROXY,BCC_HTTPS_PROXY,BCC_NO_PROXY,BOOTSTRAP_VERSION,CLUSTERID,DAS_ENVIRONMENT,DAS_ESB,DAS_LOGHOST,DAS_S3ROOT,DAS_SQS_URL,FRIENDLY_NAME,GIT_SSL_NO_VERIFY,GRB_APP_NAME,GRB_ISV_NAME,GRB_LICENSE_FILE,GUROBI_HOME,JBID,MASTER,MISSION_NAME,NO_PROXY,TEMP,TMPDIR,TZ
setup = programs.das_setup.DASDecennialSetup
spark.loglevel = ERROR
spark.name = ${DHCH_NAME}-${SINGLE_STATE_NAME}

[stats]
heartbeat_frequency = 60
notify_gc_master = 0

[takedown]
delete_output = 0
takedown = programs.takedown.takedown

[validator]
results_fname = /mnt/tmp/WNS_results
validator = programs.stub_validator.validator

[writer]
certificate_name = PR DHC Unit File
certificate_suffix = .certificate.pdf
certificate_title = Certificate of Disclosure Avoidance
certificate_person1 = Some Human
certificate_title1 = Technical Operator
certificate_person2 = Some Human
certificate_title2 = DAS Portfolio Manager
certificate_person3 = Some Human
certificate_title3 = CED Authorizing Officer
drb_clearance_number = CBDRB-FY21-DSEP-005
classification_level = C_U_I//CENS - Title 13 protected data
keep_attrs = geocode, raw, syn
multiwriter_writers = DHCH2020_MDFRevisedColumnNamesTen3Lev
num_parts = 0
output_datafile_name = $MDF_UNIT_SINGLE_STATE_NAME
output_path = $MDF_UNIT_SINGLE_STATE_PATH
overwrite_flag = 0
produce_flag = 1
s3cat = 1
s3cat_prefix = .pickle
s3cat_suffix = .txt
s3cat_verbose = 0
s3_tags = NMF=1, TAGGED=1
save_git_commit = 1
stats_dir = ${DAS_S3ROOT}/rpc/upload
upload_logfile = 1
writer = programs.writer.multi_writer.MultiWriter
write_metadata = 1
