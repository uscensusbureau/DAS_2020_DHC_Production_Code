# PLUCKY_ASPECT
[alert]
message = Hello world!

[budget]
approx_dp_delta = 1e-10
dp_mechanism = discrete_gaussian_mechanism
geolevel_budget_prop = 51/1024,153/1024,78/1024,51/1024,172/1024,519/1024
global_scale = 429/439
only_dyadic_rationals = False
privacy_framework = zcdp
query_ordering = Strategy1b_ST_CTY_BG_isoTot_Ordering
strategy = Strategy1b_St_Cty_BG_optSpine_ppmfCandidate

[constraints]
minimalschema = hhgq
theconstraints.block = hhgq_total_lb, hhgq_total_ub, nurse_nva_0
theconstraints.state = hhgq_total_lb, hhgq_total_ub
theconstraints.us = total
theinvariants.block = gqhh_vect, gqhh_tot
theinvariants.state = tot

[engine]
check_budget = off
delete_raw = 0
engine = programs.engine.topdown_engine.TopdownEngine
noisy_measurements_postfix = NMF10_PER_US
reload_noisy = 0
save_noisy = TRUE

[ENVIRONMENT]
aws_auto_scaling_home = /opt/aws/apitools/as
aws_cloudwatch_home = /opt/aws/apitools/mon
aws_elb_home = /opt/aws/apitools/elb
aws_path = /opt/aws
bcc_https_proxy = https://{HOST_NAME:PORT}
bcc_http_proxy = http://{HOST_NAME:PORT}
bcc_no_proxy = {IP_OR_HOST}
clusterid = j-33VHLGGXB25V
das_environment = ITE
das_framework_version = 1.1.0
das_loghost = {HOST_NAME} 

[environment]
das_run_uuid = a25e12a8-30c6-431e-b31d-96d4e2e6b6db

[ENVIRONMENT]
das_s3root = ${DAS_S3ROOT}
das_s3inputs = ${DAS_S3INPUTS}
friendly_name = DAS-TEST-PPMF-EPS4-0409-0612
git_ssl_no_verify = true
grb_app_name = DAS
grb_env3 = 0
grb_env4 =
grb_isv_name = Census
grb_license_file = /usr/local/lib64/python3.6/site-packages/gurobipy/gurobi_client.lic
gurobi_home = /usr/local/lib64/python3.6/site-packages/gurobipy
jbid = LightsOut
mission_name = PLUCKY_ASPECT
no_proxy = {IP_OR_HOST}
temp = /usr/tmp
tmpdir = /usr/tmp
tz = America/New_York

[error_metrics]
calculate_binned_query_errors = True
calculate_per_query_quantile_errors = True
calculate_per_query_quantile_signed_errors = True
error_metrics = programs.metrics.accuracy_metrics.AccuracyMetrics
l1_relative_error_geolevels = Place, Block_Group, OSE
l1_relative_error_queries = cenrace_7lev_two_comb * hispanic, gqlevels
population_cutoff = 500

[experiment]
run_experiment_flag = 0

[geodict]
aian_areas = Legal_Federally_Recognized_American_Indian_Area,
American_Indian_Joint_Use_Area,
Hawaiian_Home_Land,
Alaska_Native_Village_Statistical_Area,
State_Recognized_Legal_American_Indian_Area,
Oklahoma_Tribal_Statistical_Area,
Joint_Use_Oklahoma_Tribal_Statistical_Area
geocode_length = 16
geolevel_leng = 16,12,11,5,2,0
geolevel_names = Block,Block_Group,Tract,County,State,US
geo_bottomlevel = Block
geo_path =
geo_toplevel =
ignore_gqs_in_block_groups = False
spine = opt_spine

[gurobi]
barconvtol = 0.0
bariterlimit = 1000
dataindnpass_tolerancetype = opt_tol
feasibilitytol = 1e-7
gurobi_lic = /usr/local/lib64/python3.6/site-packages/gurobipy/gurobi_client.lic
gurobi_lic_create = true
gurobi_logfile_name = gurobi.log
gurobi_path = /usr/local/lib64/python3.6/site-packages/gurobipy/linux64/lib/python3.6_utf32/
l2_acceptable_statuses = OPTIMAL, SUBOPTIMAL, ITERATION_LIMIT
l2_grb_algorithm = -1
l2_grb_presolve = -1
l2_grb_presparsify = -1
l2_optimization_approach = DataIndUserSpecifiedQueriesNPass
l2_suboptimal_allowed = False
method = -1
numericfocus = 3
optimalitytol = 1e-6
opt_tol_slack = 0.1
outputflag = 1
presolve = -1
python_presolve = 1
record_gurobi_stats = True
rounder_acceptable_statuses = OPTIMAL
rounder_optimization_approach = MultipassRounder
seq_optimization_approach = L2PlusRounder_interleaved
threads = 96
threads_block = 4
threads_block_group = 8
threads_county = 32
threads_root2root = 96
threads_state = 96
threads_tract = 16
tokenserver = $GRB_TOKENSERVER
port = $GRB_TOKENSERVER_PORT

[logging]
dvs_api_endpoint = https://{HOST_NAME}/api/dvs
dvs_enabled = FALSE
logfilename = DAS
logfolder = logs
loglevel = INFO

[monitoring]
heartbeat_frequency = 60
notification_frequency = 60
notify_dashboard_gurobi_retry = true
notify_dashboard_gurobi_success = false
print_heartbeat = False
print_heartbeat_frequency = 180
send_stacktrace = False

[reader]
cenrace_das = cenrace
cenrace_das.legal = 0-62
cenrace_das.type = int
unit_table = Unit
delimiter = \t
git_commit = ami-hardening-linux commit bc660e0fdbf8d0729060c0631476835045723e74|emr-hardening-amazon-linux commit bc660e0fdbf8d0729060c0631476835045723e74|stackConfig commit bc660e0fdbf8d0729060c0631476835045723e74|das_decennial commit bc660e0fdbf8d0729060c0631476835045723e74|das_framework commit bc660e0fdbf8d0729060c0631476835045723e74|ctools commit bc660e0fdbf8d0729060c0631476835045723e74|python_dvs commit bc660e0fdbf8d0729060c0631476835045723e74|stats_2010 commit bc660e0fdbf8d0729060c0631476835045723e74|census_etl commit bc660e0fdbf8d0729060c0631476835045723e74|ctools commit bc660e0fdbf8d0729060c0631476835045723e74|dfxml commit bc660e0fdbf8d0729060c0631476835045723e74|etl_2020 commit bc660e0fdbf8d0729060c0631476835045723e74|census_etl commit bc660e0fdbf8d0729060c0631476835045723e74|ctools commit bc660e0fdbf8d0729060c0631476835045723e74|safetab-2020 commit bc660e0fdbf8d0729060c0631476835045723e74|das-vm-config commit bc660e0fdbf8d0729060c0631476835045723e74
grfc_path = ${DAS_S3INPUTS}/runs/tests/DAS-TEST-PPMF-EPS4-0409-0612/grfc/grfc_tab20_[0-9]*.txt
header = True
hhgq = qgqtyp
hhgq.legal = 0-7
hhgq.type = int
hhgq_unit_simple_recoded = qgqtyp
hhgq_unit_simple_recoded.legal = 0-7
hhgq_unit_simple_recoded.type = int
hispanic = cenhisp
hispanic.legal = 0-1
hispanic.type = int
numreaderpartitions = 10000
person.class = programs.reader.cef_2020.cef_2020_dhcp_reader.CEF2020PersonsTable
person.generated_module = programs.reader.cef_2020.cef_validator_classes
person.generated_table = CEF20_PER
person.geography = geocode
person.histogram = hhgq votingage hispanic cenrace_das
person.path = ${DAS_S3INPUTS}/runs/tests/DAS-TEST-PPMF-EPS4-0409-0612/DAS-TEST-PPMF-EPS4-0409-0612/cef/us/per/CEF20_PER_[0-9]*.txt
person.recoder = programs.reader.cef_2020.cef_2020_dhcp_reader.PL94_2020_recoder
person.recode_variables = hhgq votingage hispanic cenrace_das
main_table = Person
reader = programs.reader.table_reader.DASDecennialReader
readerpartitionlen = 14
tables = Unit Person
unit.class = programs.reader.cef_2020.cef_2020_dhcp_reader.CEF2020DHCPUnitTable
unit.generated_module = programs.reader.cef_2020.cef_validator_classes
unit.generated_table = CEF20_UNIT
unit.geography = geocode
unit.histogram = hhgq_unit_simple_recoded
unit.path = ${DAS_S3ROOT}/runs/tests/DAS-TEST-PPMF-EPS4-0409-0612/cef/us/unit/CEF20_UNIT_[0-9]*.txt
unit.recoder = programs.reader.cef_2020.cef_2020_dhcp_reader.PL94_2020_Unit_recoder
unit.recode_variables = hhgq_unit_simple_recoded
validate_input_data_constraints = True
votingage = qage
votingage.legal = 0-1
votingage.type = int

[schema]
schema = PL94_2020_SCHEMA

[setup]
environment = AWS_AUTO_SCALING_HOME,AWS_DEFAULT_REGION,AWS_CLOUDWATCH_HOME,AWS_ELB_HOME,AWS_PATH,BCC_HTTP_PROXY,BCC_HTTPS_PROXY,BCC_NO_PROXY,BOOTSTRAP_VERSION,CLUSTERID,DAS_ENVIRONMENT,DAS_JOBID,DAS_ESB,DAS_LOGHOST,DAS_S3INPUTS,DAS_S3ROOT,DAS_CLOUD,DAS_RUN_TYPE,DAS_SQS_URL,DAS_SQS_ENDPOINT,DAS_DASHBOARD_URL,FRIENDLY_NAME,GIT_SSL_NO_VERIFY,GRB_APP_NAME,GRB_ISV_NAME,GRB_LICENSE_FILE,GUROBI_HOME,GRB_TOKENSERVER,GRB_TOKENSERVER_PORT,JBID,MASTER,MISSION_NAME,NO_PROXY,TEMP,TMPDIR,TZ
setup = programs.das_setup.DASDecennialSetup
spark.loglevel = ERROR
spark.name = DHCP10-US

[stats]
heartbeat_frequency = 1
notify_gc_master = 0

[takedown]
delete_output = 0
takedown = programs.takedown.takedown

[validator]
validator = programs.stub_validator.validator

[writer]
certificate_name = A very precise data set
certificate_person1 = Some Human 
certificate_person2 = Some Human 
certificate_suffix = .certificate.pdf
certificate_title = Certificate of Disclosure Avoidance
certificate_title1 = Novice Programmer
certificate_title2 = Supervisor
classification_level = C_U_I//SP-CENS - Title 13 protected data
keep_attrs = geocode, syn, unit_syn, _invar, _cons
multiwriter_writers = BlockNodeDicts, MDFPL942020
num_parts = 100
output_datafile_name = MDF10_PER_US
output_path = ${DAS_S3ROOT}/runs/tests/DAS-TEST-PPMF-EPS4-0409-0612/DAS-TEST-PPMF-EPS4-0409-0612/mdf/us/per/MDF10_PER_US.txt
overwrite_flag = 1
produce_flag = 1
s3cat = 1
s3cat_suffix = .txt
s3cat_verbose = 0
save_git_commit = 1
stats_dir = ${DAS_S3ROOT}/rpc/upload
upload_logfile = 0
writer = programs.writer.multi_writer.MultiWriter
write_metadata = 1
