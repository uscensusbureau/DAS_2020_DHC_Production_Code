#
# This is the global defaults file
#
# Include it from the [defaults] section to include *ALL* of the sections.
# Include it from a specific section to only include the matching section.
# (e.g. an include from the [gurobi] section in a config file will only include
# the gurobi section here.)
#
# Unclear about your config file? Run "python das2020_driver.py --dump_config <configname>"
# to expanded all of the INCLUDES in your config file and dump the result.

[stats]


# Notify when GC is run on the master node
notify_gc_master: 0

[geodict]
# number of digits in a full geocode
geocode_length: 16
# Name of the level we want to start from (instead of root), set empty for root
geo_toplevel:
# Name of the level we want to end at (instead of leaf), set empty for leaf
geo_bottomlevel:
# only focus on children of this geopath, their ancestors, and their siblings
# in this case of topdown, this is exactly equivalent to retrieving a subset of the
# results (all children of this geopath). Leave blank to get everything
geo_path:

# Spine is now validated:
spine.re = ([a-z_]+)
spine.required = True
spine = non_aian_spine

# The first reccomendation from the Geography Division for the choice parameter aian_areas is:
aian_areas = Legal_Federally_Recognized_American_Indian_Area,American_Indian_Joint_Use_Area,Hawaiian_Home_Land,Alaska_Native_Village_Statistical_Area,State_Recognized_Legal_American_Indian_Area,Oklahoma_Tribal_Statistical_Area,Joint_Use_Oklahoma_Tribal_Statistical_Area


[python]
# Note: Typically you do not need to specify the executable
# because it is inherited from the PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON
# configuration variables
#executable=/mnt/apps/anaconda3-2020.07/bin/python3.8

[gurobi]
gurobi_path: $GUROBI_HOME/linux64/lib/${PYTHON_VERSION}_utf32/
gurobi_lic:  $GUROBI_HOME/gurobi_client.lic
gurobi_logfile_name: gurobi.log
OutputFlag: 1
OptimalityTol: 1e-9
BarConvTol: 1e-8
BarQCPConvTol: 0
BarIterLimit: 1000
FeasibilityTol: 1e-9
Presolve: -1
NumericFocus: 3

gurobi_lic_create=true
#TOKENSERVER={HOST_NAME}
#PORT={PORT}
TOKENSERVER=${GRB_TOKENSERVER}
PORT=${GRB_TOKENSERVER_PORT}

# Default number of threads used by Gurobi
Threads: 1

# Threads for the top-geolevel
threads_root2root: 64

# Threads for each geolevel (if not top geolevel)
threads_state: 32
threads_county: 16
threads_tract_group: 8
threads_tract: 8
threads_block_group: 8
threads_block: 8

# Do we explicitly run presolve in Python?  1 or 0
python_presolve: 1

# Record the optimizers stats in the node object.
# Recording stats automatically sends them to the dashboard, where they are archived.
# The stats are sent as a JSON object via SQS, S3 or REST,
record_gurobi_stats: True

# Gurobi prints stats to STDOUT on the CORE nodes, where it is hard to find..
# You probably don't want to do this. Ever.
print_gurobi_stats: False

# Are sub-optimal solutions acceptable in the L2/NNLS problems, or should they throw an exception?
l2_suboptimal_allowed: False

# Which algorithm should gurobi use for L2/NNLS problems?
#-1=automatic 	0=primal simplex	1=dual simplex
# 2=barrier 	3=concurrent		4=deterministic concurrent	5=deterministic concurrent simplex
l2_grb_algorithm: -1

# How aggressively should gurobi apply presolve for L2/NNLS problems?
# -1=automatic	0=off	1=conservative	2=aggressive
l2_grb_presolve: -1

# How aggressively should gurobi apply presolve sparsity reduction for L2/NNLS problems?
# -1=automatic	0=off	1=on
l2_grb_presparsify: -1

# Heartbeat notification
notification_frequency: 60

# Uncomment in your file if you want to save an LP file
# save_lp_path = $DAS_S3ROOT/lpfiles/$JBID/$MISSION_NAME
# Save every LP file for every geocode in Rhode Island that ends with a 0
# save_lp_pattern = 44*0


# Save LP files that take longer than this to solve
save_lp_seconds = 10000

# We can randomly report to the dashboard. This is mostly for testing.
# Random report frequency (0..1)
random_report_frequency=0.0

[reader]

[hdmm]
#pidentity or marginal
#strategy_type: marginal
#ps_parameters for pidentity strategy type only (needs to be same length as hist_shape)
#ps_parameters: 1,1,2,1
#save_strategy: yes
#load_strategy: yes
;load_strategy_path: $DAS_S3ROOT/users/$JBID/hdmm_RI

[writer]
writer: programs.writer.pickled_block_data_writer.PickledBlockDataWriter
#writer: programs.writer.block_node_writer.BlockNodeWriter
keep_attrs: geocode, raw, syn

# Write the Data? 0 or 1
produce_flag: 0

# S3 output:
# combine the output files? (note that we do not combine pickles)
s3cat: 1
s3cat_prefix: .pickle
s3cat_verbose: 1

# options for block_node_write
# delete existing file (if one) 0 or 1
overwrite_flag: 0

# num_parts=0 turns writer .coalesce off. For large runs should be a few thousand, for PL94-RI-like tests, 100
num_parts: 0

# Where the results go to get sent to the dashboard
stats_dir: $DAS_S3ROOT/rpc/upload

# Save the git commit in the output:
save_git_commit: 1

# Where the output of the DAS itself goes
output_path: $DAS_S3ROOT/runs/{RUN_TYPE}/$JBID/{DATE}/{DESCRIPTIVE_NAME}

# Any S3 tags to apply
s3_tags: NMF=1, TAGGED=1

# the PDF certificate!
# It has the logfile name followed by certificate_suffix.
certificate_suffix: .certificate.pdf
certificate_name: A very precise data set
certificate_title: Certificate of Disclosure Avoidance
certificate_person1 = Tech. N. Operator
certificate_title1 = Technical Operator
certificate_person2 = Some Human
certificate_title2 = DAS Portfolio Manager
certificate_person3 = Some Human
certificate_title3 = CED Authorizing Officer

[logging]
; These are all set by the das_framework/driver.py:
; loglevel: default log level
; logfolder: log output folder
; logfilename

# DVS Configuration for the data vintaging system
dvs_enabled: 0
dvs_api_endpoint: ${DAS_DASHBOARD_URL}/api/dvs


[setup]
# Specify which environment variables to copy over to workers
# It's copied over by das_framework.driver.config_apply_environment(), which takes each of these variables and adds them to the [environment] section
# The environment variables autoamtically get extracted on the workers.
environment: AWS_AUTO_SCALING_HOME,AWS_DEFAULT_REGION,AWS_CLOUDWATCH_HOME,AWS_ELB_HOME,AWS_PATH,BCC_HTTP_PROXY,BCC_HTTPS_PROXY,BCC_NO_PROXY,BOOTSTRAP_VERSION,MASTER_IP,CLUSTERID,DAS_ENVIRONMENT,DAS_JOBID,DAS_ESB,DAS_LOGHOST,DAS_S3ROOT,DAS_S3INPUTS,DAS_CLOUD,DAS_RUN_TYPE,DAS_SQS_URL,DAS_SQS_ENDPOINT,DAS_DASHBOARD_URL,FRIENDLY_NAME,GIT_SSL_NO_VERIFY,GRB_APP_NAME,GRB_ISV_NAME,GRB_LICENSE_FILE,GUROBI_HOME,JBID,MASTER,MISSION_NAME,NO_PROXY,TEMP,TMPDIR,TZ,GRB_TOKENSERVER,GRB_TOKENSERVER_PORT

# Specify the number of core nodes (with HDFS) and core nodes (without HDFS) you want when job starts up
# You must have at least 2 core nodes.
# If these are not set, then no change is made when the system starts up.
# This is implemented in the run_cluster script
# emr_task_nodes: 5
# emr_core_nodes: 5

setup: programs.das_setup.DASDecennialSetup

[ENVIRONMENT]
# Write these variables into the environment on the MASTER and WORKER nodes
#
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[budget]
only_dyadic_rationals: False

[engine]

[validator]

[takedown]
# Specify the number of core nodes (with HDFS) and task nodes (without HDFS) you want when job is done
# You must have at least 2 core nodes.
# If these are not present, no change is made when system is shut down.
# emr_task_nodes: 0
# emr_core_nodes: 2

[monitoring]
# Heartbeat just tells the user and the dashboard that we are alive
print_heartbeat: False
print_heartbeat_frequency: 180
send_stacktrace: False
heartbeat_frequency: 60


# Do we log to the dashboard on successful token acquisitions and retries?
# notifying that we got a token is just for debugging; this will typically be false:
notify_dashboard_gurobi_success: false

# notifying that we had to retry is a problem; typically this will be true:
notify_dashboard_gurobi_retry: true


# Notifications are about the current execution of the optimizer.
# It's collected from syslog.
notification_frequency: 60


[alert]
# Print this message when the system starts up
message: Hello world!

[error_metrics]
#error_metrics: programs.metrics.accuracy_metrics_workload.AccuracyMetricsWorkload
#error_metrics: programs.metrics.accuracy_metrics.AccuracyMetrics
calculate_binned_query_errors: True
