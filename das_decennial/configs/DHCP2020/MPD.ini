[DEFAULT]
# root specifies the root location for all files; testdir specifies ???; mode specifies ???
# For the demo, the root in the current directory
include=../default.ini

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[python]


[gurobi]
OutputFlag: 1
OptimalityTol: 1e-4
BarConvTol: 1e-8
BarQCPConvTol: 0
BarIterLimit: 1000
FeasibilityTol: 1e-9
Presolve: -1
NumericFocus: 3

# Method controls alg used. -1=automatic, 0=primal simplex, 1=dual simplex, 2=barrier
Method: -1

# TimeLimit: 1800
# Do we explicitly run presolve in Python?  1 or 0
python_presolve: 1

# Control the number of threads used by Gurobi
#Threads: 4
;Threads: 96
;
;# Threads for the top-geolevel
;threads_root2root: 96
;
;# Threads for each geolevel (if not top geolevel)
;threads_state: 96
;threads_county: 96
;threads_tract_group: 96
;threads_tract: 96
;threads_block_group: 96
;threads_block: 96

Threads: 96

# Threads for the top-geolevel
threads_root2root: 96

# Threads for each geolevel (if not top geolevel)
threads_state: 96
threads_county: 96
threads_tract_group: 96
threads_tract: 96
threads_block_group: 96
threads_block: 96

[geodict]
#smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,Tract_Group,County,State,US+PR
#(largest geocode length to smallest, put 0 for US or US+PR (i.e. above state) level)
geolevel_leng: 16,14,11,8,5,2,0

[setup]
setup: programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name: DAS_NAT_FULL_DHCP
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
INCLUDE=Reader/unit_2020.ini

Person.path: ${DAS_S3INPUTS}/title13_input_data/table13/
Unit.path: ${DAS_S3INPUTS}/title13_input_data/table10_20190610/

numReaderPartitions: 50000
readerPartitionLen: 14
validate_input_data_constraints: False
;partition_by_block_group: on

[engine]
engine: programs.engine.topdown_engine.TopdownEngine
geolevel_num_part: 0,0,0,10000,4000,100,1

;# Saved noisy measurements
#saved_noisy_app_id:
;
;# Saved optimized d
#saved_optimized_app_id:
;postprocess_only: off

# should we delete the true data after making DP measurements (1 for True or 0 for False)
delete_raw: 1
save_noisy: 0
reload_noisy: 0
check_budget: off

[schema]
schema: DHCP_SCHEMA

[budget]
epsilon_budget_total: 1

#budget in topdown order (e.g. US, State, .... , Block)
geolevel_budget_prop: 0.15,0.15,0.15,0.15,0.15,.15,0.1



# start with no queries
DPqueries: relgq, votingage * hispanic * cenrace, age * sex, detailed
queriesprop: .2, .5, .2,  .1


[constraints]
#the invariants created, (no spaces)
theInvariants.Block: gqhh_vect, gqhh_tot
theInvariants.State: tot

#theConstraints.Block: hhgq_total_lb, hhgq_total_ub, householder_ub, relgq0_lt15, relgq1_lt15, relgq2_lt15, relgq3_lt15, relgq4_lt15, relgq5_lt15, relgq6_gt89, relgq7_gt89, relgq8_gt89, relgq10_lt30, relgq11_gt74, relgq12_lt30, relgq13_lt15_gt89, relgq16_gt20, relgq18_lt15, relgq19_lt15, relgq20_lt15, relgq21_lt15, relgq22_lt15, relgq23_lt17_gt65, relgq24_gt25, relgq25_gt25, relgq26_gt25, relgq27_lt20, relgq31_lt17_gt65, relgq32_lt3_gt30, relgq33_lt16_gt65, relgq34_lt17_gt65, relgq35_lt17_gt65, relgq37_lt16, relgq38_lt16, relgq39_lt16_gt75, relgq40_lt16_gt75

## WARNING! ONE CONSTRAINT IS REMOVED FOR STABILITY, TO BE RESOLVED!!!
theConstraints.Block: hhgq_total_lb, hhgq_total_ub, householder_ub, relgq0_lt15, relgq1_lt15, relgq2_lt15, relgq3_lt15, relgq4_lt15, relgq5_lt15, relgq6_gt89, relgq7_gt89, relgq8_gt89, relgq10_lt30, relgq11_gt74, relgq12_lt30, relgq13_lt15_gt89, relgq16_gt20, relgq18_lt15, relgq19_lt15, relgq20_lt15, relgq21_lt15, relgq22_lt15, relgq23_lt17_gt65, relgq24_gt25, relgq25_gt25, relgq26_gt25, relgq27_lt20, relgq31_lt17_gt65, relgq32_lt3_gt30, relgq33_lt16_gt65, relgq34_lt17_gt65, relgq35_lt17_gt65, relgq37_lt16, relgq38_lt16, relgq39_lt16_gt75, spousesUnmarriedPartners_ub, people100Plus_ub, parents_ub, parentInLaws_ub

#############
theConstraints.State: total, hhgq_total_lb, hhgq_total_ub

#theConstraints.Block: relgq0_lt15, relgq1_lt15, relgq2_lt15, relgq3_lt15, relgq4_lt15, relgq5_lt15, relgq6_gt89, relgq7_gt89, relgq8_gt89, relgq10_lt30, relgq11_gt74, relgq12_lt30, relgq13_lt15_gt89, relgq16_gt20, relgq18_lt15, relgq19_lt15, relgq20_lt15, relgq21_lt15, relgq22_lt15, relgq23_lt17_gt65, relgq24_gt25, relgq25_gt25, relgq26_gt25, relgq27_lt20, relgq31_lt17_gt65, relgq32_lt3_gt30, relgq33_lt16_gt65, relgq34_lt17_gt65, relgq35_lt17_gt65, relgq37_lt16, relgq38_lt16, relgq39_lt16_gt75, relgq40_lt16_gt75
#theConstraints.State: total


minimalSchema: relgq

[writer]
#INCLUDE=Writer/default.ini
#writer: programs.writer.mdf2020writer.DHCP_MDF2020_Writer
writer: programs.writer.multi_writer.MultiWriter
multiwriter_writers: BlockNodeDicts, DHCP2020_MDF

# Where the data gets written:
output_datafile_name: MDF_PER
write_metadata: 1
s3cat: 1
s3cat_suffix: .txt
s3cat_verbose: 0

produce_flag: 1

# delete existing file (if one) 0 or 1
overwrite_flag: 1
num_parts: 5000

[validator]
validator: programs.stub_validator.validator
#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/WNS_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: 0

[experiment]
experiment: programs.experiment.experiment.experiment
run_experiment_flag: 0

[error_metrics]
#error_metrics: programs.metrics.accuracy_metrics.AccuracyMetrics
error_metrics: programs.metrics.error_metrics_stub.ErrorMetricsStub
