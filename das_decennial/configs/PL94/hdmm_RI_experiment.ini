# "Main" "fast" config to test that experiments are working
# 8/29/2019
# Some Human

[DEFAULT]
state: ri44
INCLUDE=hdmm_RI.ini

[python]
[spark]
[logging]
[ENVIRONMENT]
[geodict]
[setup]
[reader]
PersonData.path: ${DAS_S3INPUTS}/title13_input_data/table8/%(state)s.txt
UnitData.path: ${DAS_S3INPUTS}/title13_input_data/table8/%(state)s.txt
numReaderPartitions: 200
[engine]
[hdmm]
[workload]
[schema]
[budget]
[constraints]
[writer]
output_path: $DAS_S3ROOT/users/$JBID/topdown_%(state)s

[validator]
[assessment]
[takedown]
[experiment]
experiment: programs.experiment.experiment_hdmm.ExperimentHDMM

run_experiment_flag: 1

experiment_saveloc: $DAS_S3ROOT/users/$JBID/temp_expr

# we want to save the = q0,q1original data separate from the protected data; this allows us to do so
# the original data saveloc only works if the save original data flag is on (1)
save_original_data_flag: 0
original_data_saveloc: $DAS_S3ROOT/users/$JBID/experiments/original_data

# when this is turned on (1), the s3 terminal commands to recursively remove the RDD folders
# will be invoked in order to clear it out before the saveAsPickleFile function gets called
overwrite_flag: 1

filesystem: s3

budget_groups: td1, td2

num_runs: 1

# Budgets follow the order of the geolevels listed in the geodict section
# e.g. Block, Block_Group, Tract, County, State, US

td1.epsilon_budget_total = 1.0
td1.geolevel_budget_prop = 0.2, 0.2, .2, 0.2, .2
td1.workload: PL94, P1

td2.epsilon_budget_total = 2.0
td2.geolevel_budget_prop = 0.2, 0.2, .2, 0.2, .2
td2.workload: PL94

[error_metrics]
[gurobi]