# The 'main' 'testing' file, constantly used. This is the one you should use for "does it run" test

[DEFAULT]
INCLUDE=default.ini
epsilon = 1

[gurobi]
constrain_main_vars_to_zero = True

threads_root2root: 64
threads_state: 64
threads_county: 16
threads_prim: 16
threads_tract_subset: 8
threads_tract_subset_group: 8
threads_tract_group: 8
threads_tract: 5
threads_block_group: 5
threads_block: 5

OptimalityTol: 1e-9
BarConvTol: 1e-10
BarQCPConvTol: 1e-10
BarIterLimit: 1000
FeasibilityTol: 1e-9
Presolve: 2
NumericFocus: 2

# Do we explicitly run presolve in Python?  1 or 0
python_presolve: 0

# Threshold function to define which queries are "large" (and go in 1st pass), and which are "small" (and go as a sum in 1st pass and in detail in 2nd pass)
# implemented functions are:
# threshMaxTailGeo, threshBH, threshwigglesum, threshwiggleadd
threshold_function = threshwigglesum
BHfdr = 0.1
wigglesummult = 1.
maxtaildelta = 0.01

L2_acceptable_statuses=SUBOPTIMAL,ITERATION_LIMIT
Rounder_acceptable_statuses=SUBOPTIMAL,ITERATION_LIMIT

seq_optimization_approach = L2PlusRounder_interleaved
outer_pass = True
#l2_optimization_approach = DataIndUserSpecifiedQueriesNPassDecomp
#rounder_optimization_approach = MultipassRounderDecomp

l2_optimization_approach = DataIndUserSpecifiedQueriesNPassDecomp
rounder_optimization_approach = MultipassRounderDecomp
#rounder_optimization_approach = MultipassQueryRounder

DataIndNPass_toleranceType=opt_tol
test_hist_decomp=False
skip_decomp=True
opt_tol_slack = 0.
[spark]
# Whatever spark options you may have
# not currently implemented

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs
dvs_enabled: False

[environment]
; these environment variables automatically created on the MASTER and CORE nodes.

DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]
# smallest to largest (no spaces)
geolevel_names= Block,Block_Group,Tract,County,State
# Block,Block_Group,Tract,County,State
#(largest geocode length to smallest, put 0 for US or US+PR (i.e. above state) level)
geolevel_leng=16,12,11,5,2
spine=opt_spine
prim_spine=False
# The first reccomendation from the Geography Division for the choice parameter aian_areas is:
aian_areas=Legal_Federally_Recognized_American_Indian_Area,American_Indian_Joint_Use_Area,Hawaiian_Home_Land,Alaska_Native_Village_Statistical_Area,State_Recognized_Legal_American_Indian_Area,Oklahoma_Tribal_Statistical_Area,Joint_Use_Oklahoma_Tribal_Statistical_Area

target_orig_block_groups= False
target_das_aian_areas=True

[setup]
setup: programs.das_setup.DASDecennialSetup
# Spark config stuff
spark.name: DAS_RI_TEST
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
PersonData.path: $DAS_S3INPUTS/title13_input_data/table8/ri44.txt
UnitData.path:   $DAS_S3INPUTS/title13_input_data/table8/ri44.txt
;reader: programs.reader.pickled_blocks_syn2raw_reader.PickledBlockSyn2RawReader
;pickled.path: ${DAS_S3INPUTS}/users/user007/temp/data
PersonData.class: programs.reader.sql_spar_table.SQLSparseHistogramTable
UnitData.class: programs.reader.sql_spar_table.UnitFromPersonSQLSparseHistogramTable

numReaderPartitions: 1000

;measure_rdd_times: on
validate_input_data_constraints: off
partition_by_block_group: off
input_data_vintage = 2010

readerpartitionlen = 15
skip_persist_in_reader = True
# parts_after_reading changes number of partitions in the block nodes RDD:
parts_after_reading = 500

[engine]
part_size_noisy = 15
part_size_optimized = 20
part_size_optimized_block = 60

skip_persists_in_engine = True
repartition_optimized_node_rdd_evenly = True

engine: programs.engine.topdown_engine.TopdownEngine
# Sparse
#saved_noisy_app_id: application_1564059690213_0341
# Same as np.array
# saved_noisy_app_id: application_1570048278175_0034
# saved_noisy_app_id: application_1578508239190_1228

#Eps 1000
#saved_noisy_app_id: application_1580837820168_1294

# This one has saved optimized levels
## saved_noisy_app_id: application_1640720026225_0057
## postprocess_only: on
check_budget: off
;pool_measurements: on
reload_noisy: on
save_noisy: on
;noisy_measurements_postfix: noisy_measurements
;spark: off
;return_all_levels: on
;reset_dpq_weights: on

# should we delete the true data after making DP measurements (1 for True or 0 for False)
delete_raw: 0

[schema]
schema: PL94

[budget]
privacy_framework= zcdp
dp_mechanism= discrete_gaussian_mechanism
print_per_attr_epsilons= True

global_scale= 1/1
#epsilon_budget_total= 4/1
#budget in topdown order (e.g. US+PR, State, .... , Block)
geolevel_budget_prop=1/5,1/5,1/5,1/5,1/5

strategy: decomp_test_strategy
query_ordering: decomp_test_strategy_regular_ordering

;[workload]
;workload: PL94, P1

[constraints]
#the invariants created, (no spaces)
theInvariants.Block: gqhh_vect, gqhh_tot
theInvariants.State: tot

#these are the info to build cenquery.constraint objects
theConstraints.Block: hhgq_total_lb, hhgq_total_ub, nurse_nva_0
theConstraints.State: total, hhgq_total_lb, hhgq_total_ub

minimalSchema: hhgq

[writer]
writer= programs.writer.multi_writer.MultiWriter
multiwriter_writers= MDFPersonAny
#writer: programs.writer.multi_writer.MultiWriter
#multiwriter_writers: BlockNodeDicts, MDFPersonAny
# Where the data gets written:
output_path= $DAS_S3ROOT/users/$JBID/topdown_RI_RCGM_v2_4
overwrite_flag= 1
output_datafidle_name: data
produce_flag=0
keep_attrs: geocode, syn, raw_housing, raw

save_git_commit: 1

# combine output into a single file
s3cat: 1
s3cat_suffix: .txt
s3cat_verbose: 1

# num_parts=0 turns writer .coalesce off. For large runs should be a few thousand, for PL94-RI-like tests, 100
num_parts: 0

[validator]
validator: programs.stub_validator.validator
#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/RA_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: False

[experiment]
experiment= programs.experiment.config_loops_exp.ConfigLoopsExperimentByLevel
run_experiment_flag=0
loop1=FOR DEFAULT.run = 1 TO 1
loop2=FOR DEFAULT.epsilon IN 4,10,100,1000
#plotx = epsilon

[error_metrics]
calculate_binned_query_errors = True
calculate_per_query_quantile_errors = True
calculate_per_query_quantile_signed_errors = True

error_metrics = programs.metrics.accuracy_metrics.AccuracyMetrics
l1_relative_error_geolevels = Place, Block_Group, OSE
l1_relative_error_queries = cenrace_7lev_two_comb * hispanic, gqlevels

population_cutoff = 500
print_aians_l1_error_on_total_pop = True
print_place_mcd_ose_bg_l1_error_on_total_pop = True

skip_levels = Block
compute_prim_error_metrics = True
