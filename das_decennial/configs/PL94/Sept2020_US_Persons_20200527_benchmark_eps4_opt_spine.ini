# An adaptation of the main testing file for use with L1-pen Rounder, opt_tol NNLS, & Gaussian mechanism.
[DEFAULT]
INCLUDE=default.ini
epsilon = -1

[python]
executable=/mnt/apps5/intelpython3_2019.2/bin/python3.6

[gurobi]
OptimalityTol = 1e-6
BarConvTol = 1e-6
BarIterLimit = 10000
FeasibilityTol = 1e-7

# Gurobi prints stats to STDOUT on the CORE nodes, where it is hard to find..
# You probably don't want to do this.
print_gurobi_stats: False
;save_lp_path = $DAS_S3ROOT/lpfiles/$JBID/$MISSION_NAME
;save_lp_pattern = ???????????
optimization_approach = DataIndUserSpecifiedQueriesNPass
;MultiPlusWiggle
;DataIndUserSpecifiedQueriesNPass
#rounder_optimization_approach = MultipassRounder
#DataIndNPass_toleranceType = opt_tol
#const_tol_val = 25.0
#opt_tol_slack = 0.1

[spark]
# Whatever spark options you may have
# not currently implemented

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
# these environment variables automatically created on the MASTER and CORE nodes.
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]
# smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,County,State,National

#(largest geocode length to smallest, put 0 for national level) (no spaces)
geolevel_leng: 16,12,11,5,2,0

spine: non_aian_spine

[setup]
setup: programs.das_setup.DASDecennialSetup
# Spark config stuff
spark.name: ppmf_run
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
# Note: [0-6] restricts to state codes and ignores PR (72)
PersonData.path: ${DAS_S3INPUTS}/title13_input_data/table8/??[0-6]?.txt
UnitData.path: ${DAS_S3INPUTS}/title13_input_data/table8/??[0-6]?.txt
;reader: programs.reader.pickled_blocks_syn2raw_reader.PickledBlockSyn2RawReader
;pickled.path: ${DAS_S3INPUTS}/users/user007/temp/data
PersonData.class: programs.reader.sql_spar_table.SQLSparseHistogramTable
UnitData.class: programs.reader.sql_spar_table.UnitFromPersonSQLSparseHistogramTable

numReaderPartitions: 500
;measure_rdd_times: on
validate_input_data_constraints: off
partition_by_block_group: off


[engine]
engine: programs.engine.topdown_engine.TopdownEngine
geolevel_num_part: 0,0,300,5,1,0
# Sparse
#saved_noisy_app_id: application_1564059690213_0341
# Same as np.array
#saved_noisy_app_id: application_1570048278175_0034
#saved_noisy_app_id: application_1578508239190_1228
#saved_noisy_app_id: application_1580837820168_1183

# This one has saved optimized levels
;saved_noisy_app_id: application_1570048278175_0427
;postprocess_only: on
check_budget: off
;pool_measurements: on
;reload_noisy: off
save_noisy: off
;noisy_measurements_postfix: noisy_measurements
;spark: off


# should we delete the true data after making DP measurements (1 for True or 0 for False)
delete_raw: 0

[schema]
schema: PL94

[budget]
epsilon_budget_total: 4.0
#approx_dp_delta= 1e-10
#dp_mechanism=geometric_mechanism
#dp_mechanism=gaussian_mechanism
#dp_mechanism=discrete_gaussian_mechanism

#budget in topdown order (e.g. National, State, .... , Block)
geolevel_budget_prop: 0.2,0.16,0.16,0.16,0.16,0.16

# detailed query proportion of budget (a float between 0 and 1)
detailedprop: 0.025

# -- PL94-171 Persons-universe tables --
# P1 : total, numraces, cenrace
# P2 : hispanic * numraces, hispanic * cenrace
# P3 : votingage * numraces, votingage * cenrace
# P4 : votingage * numraces * hispanic, votingage * cenrace * hispanic
# P42: instlevels, gqlevels
DPqueries: total, numraces, gqlevels, cenrace, votingage * numraces * hispanic, votingage * cenrace * hispanic
queriesprop: 0.3, 0.1, 0.05, 0.1, 0.2, 0.15, 0.075
DPqueryPart0: total, numraces, gqlevels
DPqueryPart1: cenrace, votingage * numraces * hispanic
DPqueryPart2: votingage * cenrace * hispanic
DPqueryPart3: detailed

[constraints]
#the invariants created, (no spaces)
theInvariants.Block: gqhh_vect, gqhh_tot
theInvariants.State: tot

#these are the info to build cenquery.constraint objects
theConstraints.Block: hhgq_total_lb, hhgq_total_ub, nurse_nva_0
theConstraints.State: total, hhgq_total_lb, hhgq_total_ub

minimalSchema: hhgq

[writer]
writer: programs.writer.multi_writer.MultiWriter
multiwriter_writers: BlockNodeDicts
# Where the data gets written:
output_path: $DAS_S3ROOT/users/$JBID/Sept2020_US_Persons_20200527_benchmark_eps4
output_datafile_name: data
produce_flag: 1
keep_attrs: geocode, syn, raw_housing, raw

# delete existing file (if one) 0 or 1
overwrite_flag: 1
save_git_commit: 1

# combine output into a single file
s3cat: 1
s3cat_suffix: .txt
s3cat_verbose: 1
num_parts: 0

[validator]
validator: programs.stub_validator.validator
#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/ppmf_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: False

[experiment]
experiment= programs.experiment.config_loops_exp.ConfigLoopsExperimentByLevel
run_experiment_flag= 0

[error_metrics]
#error_metrics: programs.metrics.accuracy_metrics_workload.AccuracyMetricsWorkload
error_metrics= programs.metrics.accuracy_metrics.AccuracyMetrics
#calculate_binned_query_errors= False