# The 'main' 'testing' file, constantly used. This is the one you should use for "does it run" test

[DEFAULT]
INCLUDE=default.ini
epsilon = 1.0
run = 1
#opt_approach = SinglePassRegular
l2_optimization_approach = DataIndUserSpecifiedQueriesNPassPlus
rounder_optimization_approach = CellWiseRounder
run_state = ri
run_fips = 44

[python]
#executable=/mnt/apps5/intelpython3_2019.2/bin/python3.6

[gurobi]

# Threshold function to define which queries are "large" (and go in 1st pass), and which are "small" (and go as a sum in 1st pass and in detail in 2nd pass)
# implemented functions are:
# threshMaxTailGeo, threshBH, threshwigglesum, threshwiggleadd
threshold_function = threshwigglesum
BHfdr = 0.1
wigglesummult = 2.
maxtaildelta = 0.01

#optimization_approach = DataIndUserSpecifiedQueriesNPassPlus
#optimization_approach = SinglePassRegular
;TwoPassBigSmall

gurobi_logfile_name= /mnt/tmp/${JBID}gurobi.log
stats_partitions= 50

#DataIndNPass_toleranceType = const_tol
DataIndNPass_toleranceType = opt_tol
const_tol_val = 1.0
opt_tol_slack = 0.1

[spark]
# Whatever spark options you may have
# not currently implemented

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
; these environment variables automatically created on the MASTER and CORE nodes.

DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]
#smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,County,State

#(largest geocode length to smallest, put 0 for national level) (no spaces)
geolevel_leng: 16,12,11,5,2


[setup]
setup: programs.das_setup.DASDecennialSetup
# Spark config stuff
spark.name: DAS_RI_TEST
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
PersonData.path: ${DAS_S3INPUTS}/title13_input_data/table8/ri44.txt
UnitData.path: ${DAS_S3INPUTS}/title13_input_data/table8/ri44.txt
;reader: programs.reader.pickled_blocks_syn2raw_reader.PickledBlockSyn2RawReader
;pickled.path: ${DAS_S3INPUTS}/users/user007/temp/data
PersonData.class: programs.reader.sql_spar_table.SQLSparseHistogramTable
UnitData.class: programs.reader.sql_spar_table.UnitFromPersonSQLSparseHistogramTable

numReaderPartitions: 500
;measure_rdd_times: on
validate_input_data_constraints: off
partition_by_block_group: off


[engine]
engine= programs.engine.topdown_engine.TopdownEngine
check_budget= off
delete_raw= 0
#geolevel_num_part= 0,0,300,5,1
saved_noisy_app_id= application_1599576889432_0250
;postprocess_only= on
#pool_measurements= on
reload_noisy= off
save_noisy= off
noisy_measurements_postfix= noisy_measurements-eps%(epsilon)s-run%(run)s
;spark: off

[schema]
schema: PL94

[budget]
epsilon_budget_total= %(epsilon)s
#budget in topdown order (e.g. National, State, .... , Block)
geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2

DPqueries: hhgq, cenrace * hispanic * votingage, detailed
queriesprop: 0.25, 0.25, 0.5

L2_DPqueryPart0_Est = total
L2_DPqueryPart0_Qadd = hhgq
L2_DPqueryPart1_Est = hhgq
L2_DPqueryPart1_Qadd =
L2_DPqueryPart2_Est = votingage
L2_DPqueryPart2_Qadd = cenrace * hispanic * votingage
L2_DPqueryPart3_Est = detailed
L2_DPqueryPart3_Qadd = detailed

;[workload]
;workload: PL94, P1

[constraints]
#the invariants created, (no spaces)
theInvariants.Block: gqhh_vect, gqhh_tot
theInvariants.Tract: tot

#these are the info to build cenquery.constraint objects
theConstraints.Block: hhgq_total_lb, hhgq_total_ub, nurse_nva_0
theConstraints.Tract: total, hhgq_total_lb, hhgq_total_ub

minimalSchema: hhgq

[writer]
#writer: programs.writer.multi_writer.MultiWriter
#multiwriter_writers: BlockNodeDicts, MDFPersonAny
# Where the data gets written:
output_path= $DAS_S3ROOT/users/$JBID/topdown_RI_multi_improv
output_datafile_name: data
produce_flag: 0
keep_attrs: geocode, syn, raw, raw_housing

# delete existing file (if one) 0 or 1
overwrite_flag: 1
save_git_commit: 1

# combine output into a single file
s3cat: 1
s3cat_suffix: .txt
s3cat_verbose: 1
num_parts: 0

[validator]
validator: programs.stub_validator.validator
#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/RA_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: False

[experiment]
experiment= programs.experiment.config_loops_exp.ConfigLoopsExperimentEngineWriter
run_experiment_flag= 1
loop1= FOR DEFAULT.run IN 1
#loop1= FOR DEFAULT.BarConvTol IN 0.00001,0.000001,0.0000001
#loop1= FOR DEFAULT.BarConvTol IN 1e-6, 1e-7

[error_metrics]
error_metrics= programs.metrics.accuracy_metrics.AccuracyMetrics
calculate_binned_query_errors: False
queries2measure: total
